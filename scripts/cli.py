#!/usr/bin/env python3
"""
Sips and Steals Scrapy CLI

Simple command-line interface for the new Scrapy-based architecture.
Focuses on essential discovery and extraction workflows.
"""

import argparse
import subprocess
import sys
import json
import logging
from pathlib import Path
from datetime import datetime


class ScrapyCLI:
    """Simple CLI for Scrapy-based scraping system"""
    
    def __init__(self):
        self.project_dir = Path(__file__).parent.parent  # Go up from scripts/ to root
        self.data_dir = self.project_dir / 'data'
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def run_discovery(self, args):
        """Run discovery spider to find happy hour pages"""
        print("üîç Starting Restaurant Discovery...")
        print("Finding happy hour pages across restaurant websites")
        
        cmd = [
            'scrapy', 'crawl', 'discovery',
            '-s', f'RESTAURANT_DATA_FILE={self.data_dir}/restaurants.json',
            '-s', f'DISCOVERY_OUTPUT_FILE={self.data_dir}/discovered_urls.json',
            '-L', 'INFO'
        ]
        
        if args.restaurant:
            # TODO: Add restaurant filtering
            print(f"Filtering to restaurant: {args.restaurant}")
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Discovery completed successfully!")
            self._show_discovery_stats()
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Discovery failed: {e}")
            return 1
        
        return 0
    
    def run_extraction(self, args):
        """Run extraction spider to extract deals from discovered pages"""
        print("üçΩÔ∏è  Starting Deal Extraction...")
        print("Extracting happy hour deals with data-hungry approach")
        
        # Check if discovery has been run
        discovered_file = self.data_dir / 'discovered_urls.json'
        if not discovered_file.exists():
            print("‚ùå No discovered pages found. Run discovery first:")
            print("    python cli.py discover")
            return 1
        
        cmd = [
            'scrapy', 'crawl', 'extractor',
            '-s', f'INPUT_FILE={discovered_file}',
            '-s', f'DEALS_OUTPUT_FILE={self.data_dir}/deals.json',
            '-L', 'INFO'
        ]
        
        if args.restaurant:
            # TODO: Add restaurant filtering
            print(f"Filtering to restaurant: {args.restaurant}")
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Extraction completed successfully!")
            self._show_extraction_stats()
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Extraction failed: {e}")
            return 1
        
        return 0
    
    def run_profile_extraction(self, args):
        """Run deals-focused profile extraction spider"""
        print("üè¢ Starting Deals Profile Extraction...")
        print("Extracting deal-specific content (Google Places handles metadata)")
        
        cmd = [
            'scrapy', 'crawl', 'deals_profiler',
            '-s', f'INPUT_FILE={self.data_dir}/restaurants.json',
            '-s', f'PROFILES_OUTPUT_FILE={self.data_dir}/restaurant_profiles.json',
            '-L', 'INFO'
        ]
        
        if args.restaurant:
            # TODO: Add restaurant filtering
            print(f"Filtering to restaurant: {args.restaurant}")
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Profile extraction completed successfully!")
            self._show_profile_stats()
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Profile extraction failed: {e}")
            return 1
        
        return 0
    
    def run_comprehensive_pipeline(self, args):
        """Run complete discovery + extraction + profiling pipeline"""
        print("üöÄ Starting Comprehensive Pipeline...")
        print("Running discovery, deal extraction, and profile extraction")
        
        # Run discovery
        discovery_result = self.run_discovery(args)
        if discovery_result != 0:
            return discovery_result
        
        print("\n" + "="*50)
        
        # Run deal extraction
        extraction_result = self.run_extraction(args)
        if extraction_result != 0:
            return extraction_result
        
        print("\n" + "="*50)
        
        # Run profile extraction
        profile_result = self.run_profile_extraction(args)
        if profile_result != 0:
            return profile_result
        
        print("\nüéâ Comprehensive pipeline completed successfully!")
        self._show_comprehensive_summary()
        
        return 0
    
    def run_full_pipeline(self, args):
        """Run complete discovery + extraction pipeline"""
        print("üöÄ Starting Full Pipeline...")
        print("Running discovery followed by extraction")
        
        # Run discovery
        discovery_result = self.run_discovery(args)
        if discovery_result != 0:
            return discovery_result
        
        print("\n" + "="*50)
        
        # Run extraction
        extraction_result = self.run_extraction(args)
        if extraction_result != 0:
            return extraction_result
        
        print("\nüéâ Full pipeline completed successfully!")
        self._show_pipeline_summary()
        
        return 0
    
    def show_status(self, args):
        """Show current system status and statistics"""
        print("üìä Sips and Steals Scrapy Status")
        print("=" * 40)
        
        # Check for restaurant data
        restaurant_file = self.data_dir / 'restaurants.json'
        if restaurant_file.exists():
            with open(restaurant_file, 'r') as f:
                restaurant_data = json.load(f)
            
            if 'restaurants' in restaurant_data:
                restaurant_count = len(restaurant_data['restaurants'])
            else:
                restaurant_count = len(restaurant_data.get('areas', {}))
            
            print(f"üçΩÔ∏è  Restaurants loaded: {restaurant_count}")
        else:
            print("‚ùå No restaurant data found")
            return 1
        
        # Check for discovered pages
        discovered_file = self.data_dir / 'discovered_urls.json'
        if discovered_file.exists():
            with open(discovered_file, 'r') as f:
                discovered_data = json.load(f)
            
            page_count = len(discovered_data.get('pages', []))
            print(f"üîç Pages discovered: {page_count}")
            
            # Show discovery timestamp
            exported_at = discovered_data.get('exported_at', 'Unknown')
            print(f"üìÖ Last discovery: {exported_at}")
        else:
            print("‚ö†Ô∏è  No discovered pages (run discovery first)")
        
        # Check for extracted deals
        deals_file = self.data_dir / 'deals.json'
        if deals_file.exists():
            with open(deals_file, 'r') as f:
                deals_data = json.load(f)
            
            deal_count = len(deals_data.get('deals', []))
            print(f"üéØ Deals extracted: {deal_count}")
            
            # Show extraction timestamp
            exported_at = deals_data.get('exported_at', 'Unknown')
            print(f"üìÖ Last extraction: {exported_at}")
            
            # Show restaurant coverage
            restaurants_with_deals = set()
            for deal in deals_data.get('deals', []):
                if deal.get('restaurant_slug'):
                    restaurants_with_deals.add(deal['restaurant_slug'])
            
            coverage = len(restaurants_with_deals) / restaurant_count * 100
            print(f"üìà Coverage: {len(restaurants_with_deals)}/{restaurant_count} restaurants ({coverage:.1f}%)")
        else:
            print("‚ö†Ô∏è  No extracted deals (run extraction first)")
        
        # Check for restaurant profiles
        profiles_file = self.data_dir / 'restaurant_profiles.json'
        if profiles_file.exists():
            with open(profiles_file, 'r') as f:
                profiles_data = json.load(f)
            
            profile_count = len(profiles_data.get('profiles', []))
            print(f"üè¢ Restaurant profiles: {profile_count}")
            
            # Show profile timestamp
            exported_at = profiles_data.get('exported_at', 'Unknown')
            print(f"üìÖ Last profiling: {exported_at}")
            
            # Show average completeness
            profiles = profiles_data.get('profiles', [])
            if profiles:
                avg_completeness = sum(p.get('completeness_score', 0) for p in profiles) / len(profiles)
                print(f"üìä Avg profile completeness: {avg_completeness:.2f}")
        else:
            print("‚ö†Ô∏è  No restaurant profiles (run profile extraction)")
        
        return 0
    
    def analyze_results(self, args):
        """Analyze extraction results and show insights"""
        print("üß† Analyzing Extraction Results...")
        
        deals_file = self.data_dir / 'deals.json'
        if not deals_file.exists():
            print("‚ùå No deals found. Run extraction first.")
            return 1
        
        with open(deals_file, 'r') as f:
            deals_data = json.load(f)
        
        deals = deals_data.get('deals', [])
        if not deals:
            print("‚ùå No deals in file.")
            return 1
        
        print(f"üìä Analyzing {len(deals)} deals...")
        
        # Restaurant analysis
        restaurant_deals = {}
        for deal in deals:
            slug = deal.get('restaurant_slug', 'unknown')
            if slug not in restaurant_deals:
                restaurant_deals[slug] = []
            restaurant_deals[slug].append(deal)
        
        print(f"\nüèÜ Top Restaurants by Deal Count:")
        sorted_restaurants = sorted(restaurant_deals.items(), key=lambda x: len(x[1]), reverse=True)
        for i, (slug, restaurant_deals_list) in enumerate(sorted_restaurants[:10], 1):
            avg_confidence = sum(d.get('confidence_score', 0) for d in restaurant_deals_list) / len(restaurant_deals_list)
            print(f"   {i}. {slug}: {len(restaurant_deals_list)} deals (avg confidence: {avg_confidence:.2f})")
        
        # Confidence analysis
        confidences = [d.get('confidence_score', 0) for d in deals]
        avg_confidence = sum(confidences) / len(confidences)
        high_confidence = len([c for c in confidences if c >= 0.8])
        
        print(f"\nüéØ Confidence Analysis:")
        print(f"   Average confidence: {avg_confidence:.2f}")
        print(f"   High confidence deals (‚â•0.8): {high_confidence}/{len(deals)} ({high_confidence/len(deals)*100:.1f}%)")
        
        # Extraction method analysis
        methods = {}
        for deal in deals:
            method = deal.get('extraction_method', 'unknown')
            methods[method] = methods.get(method, 0) + 1
        
        print(f"\nüõ†Ô∏è  Extraction Methods:")
        for method, count in sorted(methods.items(), key=lambda x: x[1], reverse=True):
            print(f"   {method}: {count} deals")
        
        return 0
    
    def run_pricing_extraction(self, args):
        """Run menu pricing extraction on discovered pages"""
        print("üí∞ Starting Menu Pricing Extraction...")
        print("Extracting pricing data from restaurant menus and PDFs")
        
        # Check if discovered pages exist
        discovered_file = self.data_dir / 'discovered_urls.json'
        if not discovered_file.exists():
            print("‚ùå No discovered pages found. Run discovery first:")
            print("   python cli.py discover")
            return 1
        
        cmd = [
            'scrapy', 'crawl', 'menu_pricing',
            '-L', 'INFO'
        ]
        
        if args.restaurant:
            # TODO: Add restaurant filtering capability
            print(f"Filtering to restaurant: {args.restaurant}")
        
        try:
            import os
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_dir)
            result = subprocess.run(cmd, cwd=self.project_dir, check=True, env=env)
            print("‚úÖ Menu pricing extraction completed successfully!")
            
            # Show pricing summary
            self._show_pricing_summary()
            
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Menu pricing extraction failed: {e}")
            return 1
    
    def run_enrich_data(self, args):
        """Run enrich_data.py to add contact information"""
        print("üìû Starting Data Enrichment...")
        print("Adding contact information and operational details")
        
        cmd = ['python', 'scripts/enrich_data.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Data enrichment completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Data enrichment failed: {e}")
            return 1
    
    def run_fix_times(self, args):
        """Run fix_times.py to clean time data"""
        print("üïê Starting Time Data Cleanup...")
        print("Cleaning and normalizing happy hour time patterns")
        
        cmd = ['python', 'scripts/fix_times.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Time data cleanup completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Time data cleanup failed: {e}")
            return 1
    
    def run_fix_urls(self, args):
        """Run fix_urls.py to discover and repair broken URLs"""
        print("üîó Starting URL Discovery and Repair...")
        print("Testing and fixing broken restaurant URLs")
        
        cmd = ['python', 'scripts/fix_urls.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ URL discovery and repair completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå URL discovery and repair failed: {e}")
            return 1
    
    def run_district_analysis(self, args):
        """Run district_analysis.py to generate reports"""
        print("üèôÔ∏è  Starting District Analysis...")
        print("Generating comprehensive district-level reports")
        
        cmd = ['python', 'scripts/district_analysis.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ District analysis completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå District analysis failed: {e}")
            return 1
    
    def run_google_enrich(self, args):
        """Run Google Places API enrichment"""
        print("üåü Starting Google Places API Enrichment...")
        print("Enhancing restaurant data with Google's reliable business information")
        
        cmd = ['python', 'scripts/enrich_with_google_places.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Google Places enrichment completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Google Places enrichment failed: {e}")
            print("üí° Make sure GOOGLE_PLACES_API_KEY is set. See GOOGLE_PLACES_SETUP.md")
            return 1
    
    def run_google_update(self, args):
        """Run Google Places data updates"""
        update_type = args.update_type if hasattr(args, 'update_type') else 'report'
        
        print(f"üìÖ Running Google Places {update_type.title()} Update...")
        
        cmd = ['python', 'scripts/update_google_data.py', update_type]
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print(f"‚úÖ Google Places {update_type} update completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Google Places {update_type} update failed: {e}")
            print("üí° Make sure GOOGLE_PLACES_API_KEY is set. See GOOGLE_PLACES_SETUP.md")
            return 1
    
    def run_fix_addresses(self, args):
        """Run address format fixer"""
        print("üè† Starting Address Format Fix...")
        print("Converting malformed address objects to clean formatted strings")
        
        cmd = ['python', 'scripts/fix_address_format.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Address format fix completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Address format fix failed: {e}")
            return 1
    
    def run_schema_summary(self, args):
        """Run data_schema_summary.py to show architecture"""
        print("üìã Starting Schema Documentation...")
        print("Displaying data architecture and schema summary")
        
        cmd = ['python', 'scripts/data_schema_summary.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Schema summary completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Schema summary failed: {e}")
            return 1
    
    def run_restaurant_profiles(self, args):
        """Run restaurant_profiles.py to generate profiles"""
        print("üë§ Starting Restaurant Profile Generation...")
        print("Creating detailed individual restaurant profiles")
        
        cmd = ['python', 'scripts/restaurant_profiles.py']
        
        try:
            result = subprocess.run(cmd, cwd=self.project_dir, check=True)
            print("‚úÖ Restaurant profiles completed successfully!")
            return 0
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Restaurant profiles failed: {e}")
            return 1
    
    def _show_pricing_summary(self):
        """Show summary of pricing extraction results"""
        pricing_file = self.data_dir / 'menu_pricing.json'
        summary_file = self.data_dir / 'pricing_summary.json'
        
        if summary_file.exists():
            with open(summary_file, 'r') as f:
                data = json.load(f)
            
            total_restaurants = data.get('total_restaurants_with_pricing', 0)
            pricing_summary = data.get('pricing_summary', {})
            
            print(f"\nüìä Pricing Extraction Summary:")
            print(f"   Restaurants with pricing data: {total_restaurants}")
            
            # Show price range distribution
            range_counts = {}
            total_items = 0
            
            for restaurant_slug, summary in pricing_summary.items():
                price_range = summary.get('overall_price_range', '$')
                range_counts[price_range] = range_counts.get(price_range, 0) + 1
                total_items += summary.get('total_price_items', 0)
            
            print(f"   Total menu items extracted: {total_items}")
            print("\n   Price Range Distribution:")
            for price_range in ['$', '$$', '$$$', '$$$$']:
                count = range_counts.get(price_range, 0)
                if count > 0:
                    percentage = count / total_restaurants * 100
                    print(f"     {price_range}: {count} restaurants ({percentage:.1f}%)")
            
            # Show top restaurants by price item count
            if pricing_summary:
                top_restaurants = sorted(
                    pricing_summary.items(),
                    key=lambda x: x[1].get('total_price_items', 0),
                    reverse=True
                )[:3]
                
                print("\nüèÜ Top Restaurants by Menu Items Extracted:")
                for slug, summary in top_restaurants:
                    name = summary.get('restaurant_name', slug)
                    items = summary.get('total_price_items', 0)
                    price_range = summary.get('overall_price_range', '$')
                    avg_price = summary.get('overall_average_price', 0)
                    print(f"     {name}: {items} items ({price_range}, avg: ${avg_price})")
        else:
            print("\n‚ö†Ô∏è  No pricing summary found")
    
    def _show_discovery_stats(self):
        """Show discovery statistics"""
        discovered_file = self.data_dir / 'discovered_urls.json'
        if discovered_file.exists():
            with open(discovered_file, 'r') as f:
                data = json.load(f)
            
            page_count = len(data.get('pages', []))
            print(f"üìä Discovered {page_count} potentially relevant pages")
            
            # Show top restaurants by pages discovered
            restaurant_pages = {}
            for page in data.get('pages', []):
                slug = page.get('restaurant_slug', 'unknown')
                restaurant_pages[slug] = restaurant_pages.get(slug, 0) + 1
            
            if restaurant_pages:
                top_restaurants = sorted(restaurant_pages.items(), key=lambda x: x[1], reverse=True)[:5]
                print("üèÜ Top discoveries:")
                for slug, count in top_restaurants:
                    print(f"   {slug}: {count} pages")
    
    def _show_extraction_stats(self):
        """Show extraction statistics"""
        deals_file = self.data_dir / 'deals.json'
        if deals_file.exists():
            with open(deals_file, 'r') as f:
                data = json.load(f)
            
            deal_count = len(data.get('deals', []))
            print(f"üìä Extracted {deal_count} deals")
            
            # Show restaurants with deals
            restaurants_with_deals = set()
            for deal in data.get('deals', []):
                if deal.get('restaurant_slug'):
                    restaurants_with_deals.add(deal['restaurant_slug'])
            
            print(f"üçΩÔ∏è  Found deals for {len(restaurants_with_deals)} restaurants")
    
    def _show_profile_stats(self):
        """Show profile extraction statistics"""
        profiles_file = self.data_dir / 'restaurant_profiles.json'
        if profiles_file.exists():
            with open(profiles_file, 'r') as f:
                data = json.load(f)
            
            profile_count = len(data.get('profiles', []))
            print(f"üìä Extracted {profile_count} restaurant profiles")
            
            # Show completeness statistics
            profiles = data.get('profiles', [])
            if profiles:
                completeness_scores = [p.get('completeness_score', 0) for p in profiles]
                avg_completeness = sum(completeness_scores) / len(completeness_scores)
                high_completeness = len([s for s in completeness_scores if s >= 0.5])
                
                print(f"üìà Avg completeness: {avg_completeness:.2f}")
                print(f"üéØ High completeness profiles (‚â•0.5): {high_completeness}/{profile_count}")
                
                # Show top complete profiles
                profiles_by_completeness = sorted(profiles, 
                                                key=lambda x: x.get('completeness_score', 0), 
                                                reverse=True)[:5]
                print("üèÜ Most complete profiles:")
                for profile in profiles_by_completeness:
                    name = profile.get('restaurant_name', 'Unknown')
                    score = profile.get('completeness_score', 0)
                    print(f"   {name}: {score:.2f}")
    
    def _show_comprehensive_summary(self):
        """Show comprehensive pipeline summary"""
        print("\nüìà Comprehensive Pipeline Summary:")
        
        # Load all data files
        discovered_file = self.data_dir / 'discovered_urls.json'
        deals_file = self.data_dir / 'deals.json'
        profiles_file = self.data_dir / 'restaurant_profiles.json'
        
        stats = {}
        
        if discovered_file.exists():
            with open(discovered_file, 'r') as f:
                discovered_data = json.load(f)
            stats['pages'] = len(discovered_data.get('pages', []))
        
        if deals_file.exists():
            with open(deals_file, 'r') as f:
                deals_data = json.load(f)
            stats['deals'] = len(deals_data.get('deals', []))
            
            # Count restaurants with deals
            restaurants_with_deals = set()
            for deal in deals_data.get('deals', []):
                if deal.get('restaurant_slug'):
                    restaurants_with_deals.add(deal['restaurant_slug'])
            stats['restaurants_with_deals'] = len(restaurants_with_deals)
        
        if profiles_file.exists():
            with open(profiles_file, 'r') as f:
                profiles_data = json.load(f)
            stats['profiles'] = len(profiles_data.get('profiles', []))
            
            # Calculate average completeness
            profiles = profiles_data.get('profiles', [])
            if profiles:
                avg_completeness = sum(p.get('completeness_score', 0) for p in profiles) / len(profiles)
                stats['avg_completeness'] = avg_completeness
        
        # Display comprehensive stats
        if 'pages' in stats and 'deals' in stats:
            print(f"   Discovery ‚Üí Deals: {stats['pages']} pages ‚Üí {stats['deals']} deals")
        
        if 'profiles' in stats:
            print(f"   Restaurant Profiling: {stats['profiles']} comprehensive profiles")
            if 'avg_completeness' in stats:
                print(f"   Average Profile Completeness: {stats['avg_completeness']:.2f}")
        
        if 'restaurants_with_deals' in stats and 'profiles' in stats:
            print(f"   Total Restaurant Coverage: {max(stats['restaurants_with_deals'], stats['profiles'])} restaurants")
    
    def _show_pipeline_summary(self):
        """Show complete pipeline summary"""
        print("\nüìà Pipeline Summary:")
        
        # Load all data files
        discovered_file = self.data_dir / 'discovered_urls.json'
        deals_file = self.data_dir / 'deals.json'
        
        if discovered_file.exists() and deals_file.exists():
            with open(discovered_file, 'r') as f:
                discovered_data = json.load(f)
            with open(deals_file, 'r') as f:
                deals_data = json.load(f)
            
            page_count = len(discovered_data.get('pages', []))
            deal_count = len(deals_data.get('deals', []))
            
            if page_count > 0:
                extraction_rate = deal_count / page_count * 100
                print(f"   Discovery ‚Üí Extraction: {page_count} pages ‚Üí {deal_count} deals ({extraction_rate:.1f}% success rate)")
            
            # Show top performing restaurants
            restaurant_deals = {}
            for deal in deals_data.get('deals', []):
                slug = deal.get('restaurant_slug', 'unknown')
                restaurant_deals[slug] = restaurant_deals.get(slug, 0) + 1
            
            if restaurant_deals:
                top_performers = sorted(restaurant_deals.items(), key=lambda x: x[1], reverse=True)[:3]
                print("\nüèÜ Top Performers:")
                for slug, count in top_performers:
                    print(f"   {slug}: {count} deals")


def main():
    """Main CLI entry point"""
    parser = argparse.ArgumentParser(
        description='Sips and Steals Scrapy CLI - Minimal Viable Architecture',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Core Commands:
  python cli.py discover                    # Discover happy hour pages
  python cli.py extract                     # Extract deals from discovered pages
  python cli.py profile                     # Extract restaurant profiles
  python cli.py pipeline                    # Run discovery + extraction (deals only)
  python cli.py comprehensive               # Run full discovery + extraction + profiling
  python cli.py status                      # Show system status
  python cli.py analyze                     # Analyze extraction results
  python cli.py pricing                     # Extract menu pricing data

Utility Commands:
  python cli.py enrich                      # Enrich restaurant data with contact info
  python cli.py fix-times                   # Clean and normalize time data
  python cli.py fix-urls                    # Discover and repair broken URLs
  python cli.py district                    # Generate district analysis reports
  python cli.py schema                      # Show data schema summary
  python cli.py profiles                    # Generate individual restaurant profiles
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Discovery command
    discover_parser = subparsers.add_parser('discover', help='Discover happy hour pages')
    discover_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Extraction command
    extract_parser = subparsers.add_parser('extract', help='Extract deals from discovered pages')
    extract_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Profile extraction command (deals-focused)
    profile_parser = subparsers.add_parser('profile', help='Extract deal-specific content (Google Places handles metadata)')
    profile_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Pipeline command (deals only)
    pipeline_parser = subparsers.add_parser('pipeline', help='Run discovery + extraction pipeline (deals only)')
    pipeline_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Comprehensive pipeline command (deals + profiles)
    comprehensive_parser = subparsers.add_parser('comprehensive', help='Run full discovery + extraction + deal profiling pipeline')
    comprehensive_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Status command
    status_parser = subparsers.add_parser('status', help='Show system status')
    
    # Analysis command
    analyze_parser = subparsers.add_parser('analyze', help='Analyze extraction results')
    
    # Pricing command  
    pricing_parser = subparsers.add_parser('pricing', help='Run menu pricing extraction on discovered pages')
    pricing_parser.add_argument('--restaurant', help='Filter to specific restaurant slug')
    
    # Utility commands
    enrich_parser = subparsers.add_parser('enrich', help='Enrich restaurant data with contact information')
    
    fix_times_parser = subparsers.add_parser('fix-times', help='Clean and normalize time data')
    
    fix_urls_parser = subparsers.add_parser('fix-urls', help='Discover and repair broken restaurant URLs')
    
    district_parser = subparsers.add_parser('district', help='Generate district-level analysis reports')
    
    schema_parser = subparsers.add_parser('schema', help='Show data schema summary and architecture')
    
    profiles_parser = subparsers.add_parser('profiles', help='Generate individual restaurant profiles')
    
    # Google Places API commands
    google_enrich_parser = subparsers.add_parser('google-enrich', help='Enrich restaurants with Google Places API data')
    
    google_update_parser = subparsers.add_parser('google-update', help='Update Google Places data (daily/weekly/monthly/report)')
    google_update_parser.add_argument('update_type', nargs='?', default='report', 
                                     choices=['daily', 'weekly', 'monthly', 'report', 'check'],
                                     help='Type of update to perform (default: report)')
    
    fix_addresses_parser = subparsers.add_parser('fix-addresses', help='Fix malformed address format issues')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    cli = ScrapyCLI()
    
    # Route to appropriate method
    if args.command == 'discover':
        return cli.run_discovery(args)
    elif args.command == 'extract':
        return cli.run_extraction(args)
    elif args.command == 'profile':
        return cli.run_profile_extraction(args)
    elif args.command == 'pipeline':
        return cli.run_full_pipeline(args)
    elif args.command == 'comprehensive':
        return cli.run_comprehensive_pipeline(args)
    elif args.command == 'status':
        return cli.show_status(args)
    elif args.command == 'analyze':
        return cli.analyze_results(args)
    elif args.command == 'pricing':
        return cli.run_pricing_extraction(args)
    elif args.command == 'enrich':
        return cli.run_enrich_data(args)
    elif args.command == 'fix-times':
        return cli.run_fix_times(args)
    elif args.command == 'fix-urls':
        return cli.run_fix_urls(args)
    elif args.command == 'district':
        return cli.run_district_analysis(args)
    elif args.command == 'schema':
        return cli.run_schema_summary(args)
    elif args.command == 'profiles':
        return cli.run_restaurant_profiles(args)
    elif args.command == 'google-enrich':
        return cli.run_google_enrich(args)
    elif args.command == 'google-update':
        return cli.run_google_update(args)
    elif args.command == 'fix-addresses':
        return cli.run_fix_addresses(args)
    else:
        parser.print_help()
        return 1


if __name__ == '__main__':
    sys.exit(main())